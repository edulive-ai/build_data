import os
import cv2
import json
import fitz  # PyMuPDF
import numpy as np
from doclayout_yolo import YOLOv10
from huggingface_hub import hf_hub_download

# === C·∫•u h√¨nh ===
IMG_PATH = "/home/batien/Desktop/build_data/toan1_tuduy_1001_images/To√°n 1 Ch√¢n tr·ªùi s√°ng t·∫°o_page_014.png"
CROP_DIR = "crop_images1"
os.makedirs(CROP_DIR, exist_ok=True)

# === Tham s·ªë l·ªçc ===
MIN_AREA = 130000  # 629 * 205 = 128,945
MIN_Y = 95
MAX_Y = 7950

print(f"üîß C·∫•u h√¨nh l·ªçc:")
print(f"   - Di·ªán t√≠ch t·ªëi thi·ªÉu: {MIN_AREA:,} pixels")
print(f"   - V√πng Y h·ª£p l·ªá: {MIN_Y} <= y <= {MAX_Y}")

# === Load model YOLO ===
filepath = hf_hub_download(
    repo_id="juliozhao/DocLayout-YOLO-DocStructBench",
    filename="doclayout_yolo_docstructbench_imgsz1024.pt"
)
model = YOLOv10(filepath)

# === D·ª± ƒëo√°n ===
det_res = model.predict(IMG_PATH, imgsz=1024, conf=0.5, device="cuda")

# === D√πng OpenCV ƒë·ªÉ l·∫•y k√≠ch th∆∞·ªõc ·∫£nh YOLO resize (1024x...)
yolo_input = cv2.imread(IMG_PATH)
yolo_h, yolo_w = yolo_input.shape[:2]

# === D√πng fitz ƒë·ªÉ ƒë·ªçc ·∫£nh ch·∫•t l∆∞·ª£ng cao
doc = fitz.open(IMG_PATH)
pix = doc[0].get_pixmap(dpi=300)  # ·∫£nh g·ªëc c√≥ th·ªÉ r·∫•t l·ªõn
fitz_img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, pix.n)
if pix.n == 4:  # n·∫øu c√≥ alpha
    fitz_img = cv2.cvtColor(fitz_img, cv2.COLOR_RGBA2RGB)
fitz_h, fitz_w = fitz_img.shape[:2]

# === T√≠nh h·ªá s·ªë scale
scale_x = fitz_w / yolo_w
scale_y = fitz_h / yolo_h

print(f"üìè K√≠ch th∆∞·ªõc ·∫£nh:")
print(f"   - YOLO input: {yolo_w}x{yolo_h}")
print(f"   - ·∫¢nh g·ªëc: {fitz_w}x{fitz_h}")
print(f"   - Scale: x={scale_x:.3f}, y={scale_y:.3f}")

# === X·ª≠ l√Ω v√† l·ªçc bbox
valid_bboxes = []
filtered_bboxes = []
valid_count = 0

for i, result in enumerate(det_res[0].boxes):
    x1, y1, x2, y2 = result.xyxy[0].tolist()
    
    # Scale v·ªÅ t·ªça ƒë·ªô ·∫£nh g·ªëc
    scaled_x1 = int(x1 * scale_x)
    scaled_y1 = int(y1 * scale_y)
    scaled_x2 = int(x2 * scale_x)
    scaled_y2 = int(y2 * scale_y)
    
    # T√≠nh k√≠ch th∆∞·ªõc
    width = scaled_x2 - scaled_x1
    height = scaled_y2 - scaled_y1
    area = width * height
    
    bbox_info = {
        "original_index": i,
        "class_id": int(result.cls[0]),
        "confidence": float(result.conf[0]),
        "bbox": [scaled_x1, scaled_y1, scaled_x2, scaled_y2],
        "width": width,
        "height": height,
        "area": area
    }
    
    # === Ki·ªÉm tra ƒëi·ªÅu ki·ªán l·ªçc ===
    is_valid = True
    reject_reasons = []
    
    # Ki·ªÉm tra di·ªán t√≠ch
    if area < MIN_AREA:
        is_valid = False
        reject_reasons.append(f"di·ªán t√≠ch nh·ªè ({area:,} < {MIN_AREA:,})")
    
    # Ki·ªÉm tra v·ªã tr√≠ Y
    if scaled_y1 < MIN_Y:
        is_valid = False
        reject_reasons.append(f"y1 qu√° nh·ªè ({scaled_y1} < {MIN_Y})")
    
    if scaled_y1 > MAX_Y:
        is_valid = False
        reject_reasons.append(f"y1 qu√° l·ªõn ({scaled_y1} > {MAX_Y})")
    
    if is_valid:
        # === Crop, l√†m n√©t v√† l∆∞u bbox h·ª£p l·ªá ===
        cropped = fitz_img[scaled_y1:scaled_y2, scaled_x1:scaled_x2]
        
        # L√†m n√©t ·∫£nh b·∫±ng kernel sharpen
        sharpen_kernel = np.array([[0, -1, 0],
                                   [-1, 5, -1],
                                   [0, -1, 0]])
        sharpened = cv2.filter2D(cropped, -1, sharpen_kernel)
        
        # L∆∞u v·ªõi ch·ªâ s·ªë m·ªõi (ƒë√°nh s·ªë l·∫°i t·ª´ 0)
        out_path = os.path.join(CROP_DIR, f"crop_{valid_count}_cls{int(result.cls[0])}.png")
        cv2.imwrite(out_path, sharpened)
        
        bbox_info["new_index"] = valid_count
        bbox_info["output_path"] = out_path
        valid_bboxes.append(bbox_info)
        
        print(f"‚úÖ Bbox {i} -> crop_{valid_count}: [{scaled_x1}, {scaled_y1}, {scaled_x2}, {scaled_y2}] "
              f"(W:{width}, H:{height}, Area:{area:,}) -> {out_path}")
        valid_count += 1
        
    else:
        # Bbox b·ªã lo·∫°i b·ªè
        bbox_info["reject_reasons"] = reject_reasons
        filtered_bboxes.append(bbox_info)
        
        print(f"‚ùå Bbox {i} B·ªä LO·∫†I: [{scaled_x1}, {scaled_y1}, {scaled_x2}, {scaled_y2}] "
              f"(W:{width}, H:{height}, Area:{area:,}) - L√Ω do: {', '.join(reject_reasons)}")

# === Th·ªëng k√™ k·∫øt qu·∫£ ===
total_boxes = len(det_res[0].boxes)
print(f"\nüìä Th·ªëng k√™:")
print(f"   - T·ªïng bbox ban ƒë·∫ßu: {total_boxes}")
print(f"   - Bbox h·ª£p l·ªá: {len(valid_bboxes)}")
print(f"   - Bbox b·ªã lo·∫°i: {len(filtered_bboxes)}")
print(f"   - T·ª∑ l·ªá gi·ªØ l·∫°i: {len(valid_bboxes)/total_boxes*100:.1f}%")

# === L∆∞u JSON k·∫øt qu·∫£ ===
result_data = {
    "config": {
        "min_area": MIN_AREA,
        "min_y": MIN_Y,
        "max_y": MAX_Y,
        "scale_x": scale_x,
        "scale_y": scale_y
    },
    "statistics": {
        "total_boxes": total_boxes,
        "valid_boxes": len(valid_bboxes),
        "filtered_boxes": len(filtered_bboxes),
        "retention_rate": len(valid_bboxes)/total_boxes*100
    },
    "valid_bboxes": valid_bboxes,
    "filtered_bboxes": filtered_bboxes
}

with open("bounding_boxes_filtered.json", "w", encoding='utf-8') as f:
    json.dump(result_data, f, ensure_ascii=False, indent=2)

print(f"üíæ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o: bounding_boxes_filtered.json")

# === Annotate ·∫£nh (ch·ªâ hi·ªÉn th·ªã bbox h·ª£p l·ªá) ===
annotated = det_res[0].plot(pil=True, line_width=5, font_size=20)
cv2.imwrite("result_annotated_all.jpg", cv2.cvtColor(np.array(annotated), cv2.COLOR_RGB2BGR))

# T·∫°o ·∫£nh annotate ch·ªâ v·ªõi bbox h·ª£p l·ªá
yolo_img_copy = yolo_input.copy()
for bbox_info in valid_bboxes:
    # Scale ng∆∞·ª£c v·ªÅ t·ªça ƒë·ªô YOLO ƒë·ªÉ v·∫Ω
    x1 = int(bbox_info["bbox"][0] / scale_x)
    y1 = int(bbox_info["bbox"][1] / scale_y)
    x2 = int(bbox_info["bbox"][2] / scale_x)
    y2 = int(bbox_info["bbox"][3] / scale_y)
    
    # V·∫Ω bbox
    cv2.rectangle(yolo_img_copy, (x1, y1), (x2, y2), (0, 255, 0), 3)
    cv2.putText(yolo_img_copy, f"cls{bbox_info['class_id']}", 
                (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

cv2.imwrite("result_annotated_valid_only.jpg", yolo_img_copy)
print(f"üñºÔ∏è  ƒê√£ l∆∞u ·∫£nh: result_annotated_all.jpg (t·∫•t c·∫£ bbox)")
print(f"üñºÔ∏è  ƒê√£ l∆∞u ·∫£nh: result_annotated_valid_only.jpg (ch·ªâ bbox h·ª£p l·ªá)")

print(f"\nüéØ Ho√†n th√†nh! ƒê√£ l∆∞u {len(valid_bboxes)} ·∫£nh crop v√†o th∆∞ m·ª•c '{CROP_DIR}'")